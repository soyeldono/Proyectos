{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PWRMkNyslkd"
   },
   "source": [
    "# Prediccion Click-through con Arboles de Decision\n",
    "\n",
    "> **Proyecto a realizar** : Aplicar el estimador RandomForestClassifier para solucionar el problema de prediccion CTR sobre el conjunto de datos Avazu obteniendo un minimo de 80% de score AUC, sobre una validacion cruzada de 5 partes.\n",
    "\n",
    "-  Utilizar busqueda en malla de 5 partes (GridSearchCV)\n",
    "-  Buscar documentacion de la clase RandomForestClassifier para conocer los parametros que recibe el constructor de la misma.\n",
    "-  Incluir en la busqueda en malla 4 de estos parametros (como mınimo) y 40 combinaciones de parametros y valores (tambien como mınimo).\n",
    "- Graficar los atributos mas importantes determinados por el clasificador.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTA:\n",
    "\n",
    "Este archivo fue hecho en equipo pero para manterner la privacidad de los integrantes se quitaron sus datos, a la par que se quitaron la mayoria de las casillas ya que eran demasiadas y solamente se dejaron las importantes, esto con el fin de comprobar su funcionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z6ndYz2Dz5lm"
   },
   "source": [
    "## LIBRERIAS QUE VAMOS A USAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ccn4bFaEqtzZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "import csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import copy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJKKyWUXskLm"
   },
   "source": [
    "---\n",
    "## Funciones que vamos a utilizar para relizar el proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJ4INwJD0MaW"
   },
   "outputs": [],
   "source": [
    "def read_ad_click_data(filename, n, offset=0):\n",
    "    \"\"\" Read training instances from the Click-Through Rate Prediction dataset\n",
    "       (https://www.kaggle.com/c/avazu-ctr-prediction)\n",
    "    Args:\n",
    "        filename - Name of file to read instances from\n",
    "        n - Number of instances to read\n",
    "        offset - Instance number to begin reading from\n",
    "    Returns:\n",
    "        X_dict, y - Features of the read instances (as a dictionary) and corresponding labels (as a list)\n",
    "        \n",
    "    Note:\n",
    "        We exclude features 'id', 'hour', 'device_id', and 'device_ip' from the dataset in order to avoid\n",
    "        a huge number of features coming from the OHE of these categorical attributes\n",
    "    \"\"\"\n",
    "    X_dict, y = [], []\n",
    "    \n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        for i in range(offset):\n",
    "            next(reader)\n",
    "            \n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            i += 1\n",
    "            y.append(int(row['click']))\n",
    "            del row['click'], row['id'], row['hour'], row['device_id'], row['device_ip']\n",
    "            X_dict.append(dict(row))\n",
    "            \n",
    "            if i >= n:\n",
    "                break\n",
    "                \n",
    "    return X_dict, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfuYxATO2Hod"
   },
   "source": [
    "---\n",
    "## Leemos los datos de los archivos CSV  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e_GN3ErA1kHN"
   },
   "outputs": [],
   "source": [
    "n_max =10000 # can be up to 100,000 instances (but requieres A LOT of memory to process)5\n",
    "X_dict_train, y_train = read_ad_click_data('train_ctr.csv', n_max)\n",
    "X_dict_test, y_test = read_ad_click_data('test_ctr.csv', n_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TGej4ecy3BlV"
   },
   "source": [
    "---\n",
    "## Declaramos nuestras variables de entrenamiento y de prueba aplicando el OHE\n",
    "\n",
    "Usamos el OHE debido a que hay algunas columnas que no son numericas, a lo cual nos impide trabajar con ellas a lo menos que la pasemos a formato numerico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lFG7vsHk3AUU"
   },
   "outputs": [],
   "source": [
    "dict_one_hot_encoder = DictVectorizer(sparse=False)\n",
    "X_train = dict_one_hot_encoder.fit_transform(X_dict_train)\n",
    "X_test = dict_one_hot_encoder.transform(X_dict_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n7WMTK4T361l"
   },
   "source": [
    "---\n",
    "## Visualizamos los datos\n",
    "\n",
    "Si todo salio bien durante esta parte del proyecto, entonces tendriamos en nuestras variables *$ X-dict-train,X-dict-test $* 30,000 datos y 19 columnas. Y por la parte de  *$ X-train,X-test $* deberiamos tener mucho mas columnas 19 debiado al OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1557196484097,
     "user": {
      "displayName": "Josué Soto Cortez",
      "photoUrl": "https://lh3.googleusercontent.com/-4U4su2CJ6n8/AAAAAAAAAAI/AAAAAAAAtuQ/aUs4ipYvDno/s64/photo.jpg",
      "userId": "00274054752953513847"
     },
     "user_tz": 300
    },
    "id": "Ybc1pOfF3Xo9",
    "outputId": "0c502768-a686-495c-f927-1f27fe4fa668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_Datos: 10000 Num_Columnas: 19\n",
      "---- APLICACION DEL OHE ----\n",
      "Num_colmnas despues de aplicar OHE: 2820\n"
     ]
    }
   ],
   "source": [
    "print(\"Num_Datos:\",len(X_dict_train), \"Num_Columnas:\",len(X_dict_train[0]))\n",
    "print(\"---- APLICACION DEL OHE ----\")\n",
    "print(\"Num_colmnas despues de aplicar OHE:\",len(X_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vpEXnTmk8iQF"
   },
   "source": [
    "---\n",
    "**$$  ANALISIS {\\thinspace} DE {\\thinspace} LOS {\\thinspace} DATOS  $$**\n",
    "\n",
    "> Antes de continuar debemos de hacer un pequeño analisis sobre que tenemos que hacer y que podemos hacer ahora para evitar \"trabajar de mas\". Lo que nos queremos referir con esto es, que casi siempre al momento de que alguien aplica **OHE** y quiere obtener una prediccion o lo que sea, tiende a generarse *$ruido$*. A lo que hemos optado por aplicar **Lasso** ya que este metodo nos permite saber cuales columnas de nuestros conjuntos son \"inutiles\" o no tiene sentido tomarlas en cuenta.\n",
    " \n",
    "\n",
    " **Recordatorio de Lasso:**  Para poder usar Lasso haremos uso de la libreria Sklearn que ya nos proporciona dicho metodo, mas sin embargo cabe recordar que Lasso tiene un parametro llamado ** alpha ** dicho parametro es necesario encontrar el optimo para evitar que Lasso no sea ni muy estricto ni tampoco que sea muy ligero al momento de evaluar las columnas.\n",
    " \n",
    "**Aplicacion de  Lasso:** Entonces para poder aplicar Lasso como ya habiamos mencionado previamente, vamos a hacer uso de la libreria Sklearn, pero aparte de hacer uso de Lasso tambien vamos a necesitar de la **Validacion Cruzada** o tambien conocida por su nombre en ingles **Cross Val Socre**, el codigo de ambos se escribe de la siguiente manera:\n",
    "\n",
    "\n",
    "\n",
    "``` python\n",
    "\n",
    "# Para usar Lasso\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    ">>> varaible_lasso = Lasso(alpha=n) # <- Aqui decimos que vamos a usar lasso con una alpha=n donde n es un 'float'\n",
    ">>> variable_lasso.fit(X_train,y_train) # <- Aqui le decimos que aplique lasso a nuestros dos conjuntos\n",
    "\n",
    "# Para usar Cross Val Score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    ">>> variable_CVS = cross_val_score(metodo, X, y, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "En el cual vamos a estar moviento los valores de **alpha** para encontrar el menor error posible segun Lasso, pero el \"problema\" que nos puede pasar ahora es que, nuestras varibale **X_train** al tener 30,000 datos  y mas de 4000 columnas, este proceso le va a llevar mucho tiempo a Lasso (en nuestro caso se tardaba 10min aproximadamente, cada que cambiamos el parametro **alpha**).  Asi que se les recomienda que apartir de aqui en adelante se tenga un buen equipo computacional para poder ejecutuar las lineas de codigos siguientes. (En nuestro caso a los mas que llego a usar fue 8gb de ram debido a que pusimos muchos datos y muchas **alphas**)\n",
    "\n",
    "Lo que vamos a hacer ahora sera hacer pruebas con diferentes **alphas** y al final mostraremos una tabla para comparar resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q9TF_wq-RZNy"
   },
   "source": [
    "### $$ Para {\\thinspace} alpha {\\thinspace} = {\\thinspace} 0.1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4593,
     "status": "ok",
     "timestamp": 1557196494550,
     "user": {
      "displayName": "Josué Soto Cortez",
      "photoUrl": "https://lh3.googleusercontent.com/-4U4su2CJ6n8/AAAAAAAAAAI/AAAAAAAAtuQ/aUs4ipYvDno/s64/photo.jpg",
      "userId": "00274054752953513847"
     },
     "user_tz": 300
    },
    "id": "dZEOnyZzRbhg",
    "outputId": "7297069c-012d-42c5-909c-c8ed30cf44b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.376152776744873\n"
     ]
    }
   ],
   "source": [
    "lasso_reg = Lasso(alpha=0.1)\n",
    "scores = cross_val_score(lasso_reg, X_train, y_train,scoring=\"neg_mean_squared_error\", cv=5)\n",
    "rmse_scores = np.sqrt( - scores)\n",
    "print(np.mean(rmse_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QNUd6OoiRTVr"
   },
   "source": [
    "### $$ Para {\\thinspace} alpha {\\thinspace} = {\\thinspace} 0.01 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6644,
     "status": "ok",
     "timestamp": 1557196507013,
     "user": {
      "displayName": "Josué Soto Cortez",
      "photoUrl": "https://lh3.googleusercontent.com/-4U4su2CJ6n8/AAAAAAAAAAI/AAAAAAAAtuQ/aUs4ipYvDno/s64/photo.jpg",
      "userId": "00274054752953513847"
     },
     "user_tz": 300
    },
    "id": "RYUuKgiIRI95",
    "outputId": "f2c18e29-fc02-4210-d6fd-9cbfe211eb43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3691060783955664\n"
     ]
    }
   ],
   "source": [
    "lasso_reg = Lasso(alpha=0.01)\n",
    "scores = cross_val_score(lasso_reg, X_train, y_train,scoring=\"neg_mean_squared_error\", cv=5)\n",
    "rmse_scores = np.sqrt( - scores)\n",
    "print(np.mean(rmse_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SQgh46PhRE4B"
   },
   "source": [
    "### $$ Para {\\thinspace} alpha {\\thinspace} = {\\thinspace} 0.001 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17679,
     "status": "ok",
     "timestamp": 1557196526811,
     "user": {
      "displayName": "Josué Soto Cortez",
      "photoUrl": "https://lh3.googleusercontent.com/-4U4su2CJ6n8/AAAAAAAAAAI/AAAAAAAAtuQ/aUs4ipYvDno/s64/photo.jpg",
      "userId": "00274054752953513847"
     },
     "user_tz": 300
    },
    "id": "d4y4PLqcKBFC",
    "outputId": "e0863cc0-b0e5-4ba6-fd4b-b103e122695d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36196876020902113\n"
     ]
    }
   ],
   "source": [
    "lasso_reg = Lasso(alpha=0.001)\n",
    "scores = cross_val_score(lasso_reg, X_train, y_train,scoring=\"neg_mean_squared_error\", cv=5)\n",
    "rmse_scores = np.sqrt( - scores)\n",
    "print(np.mean(rmse_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Roi1R81SQpuy"
   },
   "source": [
    "### $$ Para {\\thinspace} alpha {\\thinspace} = {\\thinspace} 0.0001 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 137092,
     "status": "ok",
     "timestamp": 1557196672458,
     "user": {
      "displayName": "Josué Soto Cortez",
      "photoUrl": "https://lh3.googleusercontent.com/-4U4su2CJ6n8/AAAAAAAAAAI/AAAAAAAAtuQ/aUs4ipYvDno/s64/photo.jpg",
      "userId": "00274054752953513847"
     },
     "user_tz": 300
    },
    "id": "E2uK-kc0-ups",
    "outputId": "bbe05564-b5f6-4b47-eb8a-86b7ea880d14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36196661886901593\n"
     ]
    }
   ],
   "source": [
    "lasso_reg = Lasso(alpha=0.0001)\n",
    "scores = cross_val_score(lasso_reg, X_train, y_train,scoring=\"neg_mean_squared_error\", cv=5)\n",
    "rmse_scores = np.sqrt( - scores)\n",
    "print(np.mean(rmse_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQn3Vc4fS8lL"
   },
   "source": [
    "## $$ TABLA{\\thinspace}{\\thinspace}COMPARATIVA  $$\n",
    "\n",
    "Despues de haber aplicado Lasso con multplies **alphas** diferentes, es momento de saber cual es el mejor **alpha** para nuestro proyecto, para eso usaremos la tabal de abajo, donde nos dice el **alpha** que hayamos usado, la cantidad de datos (esto en el debido caso de querer trabajar en diferentes conjuntos) y finalmente su error o tambien conocido como $RMSE$. Donde al analizar esta tabla llegamos a la conclusion clara de que el mejor parametro que podemos usar es **alpha = 0.0001**\n",
    "\n",
    "\n",
    "|  alpha    |  Cantidad Datos     | RMSE  |\n",
    "|-------   |:--------------:|-------------------:|\n",
    "|0.0001|10,000|0.3610409598229232|\n",
    "|0.001|10,000|0.3619687602090211|\n",
    "|0.01|10,000  |0.3691060783955663|\n",
    "|0.1|10,000|0.376152776744873|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UobsHx4Wakq"
   },
   "source": [
    "---\n",
    "## $$ ACTUALIZACION{\\thinspace}DE{\\thinspace}LOS{\\thinspace}DATOS $$\n",
    "\n",
    "> Una vez que encontramos el mejor alpha para Lasso, ahora toca quitar las columnas que dijo Lasso que no tienen sentido usar y despues actualizar los datos. Para ello usaremos algo que tambien ya lo tiene implementado Sklearn y es ** coef_ **, la cual nos trae el valor que le dio Lasso a cada columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33596,
     "status": "ok",
     "timestamp": 1557196789920,
     "user": {
      "displayName": "Josué Soto Cortez",
      "photoUrl": "https://lh3.googleusercontent.com/-4U4su2CJ6n8/AAAAAAAAAAI/AAAAAAAAtuQ/aUs4ipYvDno/s64/photo.jpg",
      "userId": "00274054752953513847"
     },
     "user_tz": 300
    },
    "id": "XqdrwzdYT3Zo",
    "outputId": "02bb2fa3-b447-4363-ff00-0b1c13e30255"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.0001, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primero aplicamos el mejor alpha a lasso a los conjuntos X_train,y_train\n",
    "lasso_r = Lasso(alpha=0.0001)\n",
    "lasso_r.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "joecX_x2ZuXJ"
   },
   "source": [
    "#### ---Una vez aplicado Lasso a nuestros conjuntos de entrenamiento, tenemos que saber cuales columnas si nos interesan para nuestro proyecto, para ello haremos lo siguiente---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wu_WY8rhZ4uY"
   },
   "outputs": [],
   "source": [
    "index_lasso = lasso_r.coef_\n",
    "sirve = [] # Guardaremos las columnas que si nos interesan\n",
    "for i,j in enumerate(index_lasso): #Recorremos toda la lista\n",
    "    if j != 0:\n",
    "        sirve.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nzaGhUqzcFwA"
   },
   "outputs": [],
   "source": [
    "parameters = {'n_estimators': [130],'max_depth':[37,40], 'min_samples_split': [313,314,315], 'max_leaf_nodes': [141,145,None]}\n",
    "random_forest = RandomForestClassifier(criterion='gini',n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "fROdQUMifkJ_",
    "outputId": "b1661fa2-f53d-4fa3-c79a-8196467dd0ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [130], 'max_depth': [37, 40], 'min_samples_split': [313, 314, 315], 'max_leaf_nodes': [141, 145, None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(random_forest, parameters, n_jobs=-1, cv=5, scoring='roc_auc', iid=True, verbose=2)\n",
    "grid_search.fit(X_train[:,sirve], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qoi2yTrSfs_H",
    "outputId": "44252741-7a90-4059-9d6c-a581cbdecf08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 37,\n",
       " 'max_leaf_nodes': 141,\n",
       " 'min_samples_split': 314,\n",
       " 'n_estimators': 130}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImCMkPIOHpqV"
   },
   "outputs": [],
   "source": [
    "randomforest_best = grid_search.best_estimator_\n",
    "pos_prob = randomforest_best.predict_proba(X_test[:,sirve])[:, 1] # column 0 - Negative class; column 1 - Positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bzk-VN58Hpqa",
    "outputId": "e53f1bc0-06e7-43da-c738-e4211b554bc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC AUC on testing set is: 0.722\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print('The ROC AUC on testing set is: {0:.3f}'.format(roc_auc_score(y_test, pos_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
